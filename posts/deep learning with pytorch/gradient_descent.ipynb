{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.24.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "print(np.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "维基百科定义: 梯度下降是一种用于寻找可微函数局部最小值的一阶迭代优化算法。  \n",
    "我们的定义: 梯度下降是机器学习和深度学习中常用的一种迭代技术。它从初始(可能是随机或猜测的)数据开始,为给定模型、数据点和损失函数,找到可能的最佳参数/系数。\n",
    "# 可视化\n",
    "## 模型\n",
    "从最简单的模型入手 -- 简单线性回归  \n",
    "$ y = b + wx + \\epsilon $  \n",
    "在这模型里，我们用feature(x)来predict label(y). b参数为bias(intercept), w参数为weight(slope), $\\epsilon$为noise(error)  \n",
    "我们可以很快构造出这样的一个模型结构，比如  \n",
    "    工资 = 底薪 + 年增长 * 工作年限 + noise  \n",
    "## 数据生成\n",
    "取b=1,w=2,用np.random生成随机误差$\\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_b = 1\n",
    "true_w = 2\n",
    "N = 100\n",
    "\n",
    "np.random.seed(42)\n",
    "x = np.random.rand(N,1)\n",
    "epsilon = .1 * np.random.randn(N,1)\n",
    "y = true_b + true_w * x + epsilon\n",
    "print(x,epsilon,y)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来八二开成训练数据和验证(测试)数据集，并打乱。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.29122914],\n",
       "       [0.54671028],\n",
       "       [0.66252228],\n",
       "       [0.73199394],\n",
       "       [0.60111501],\n",
       "       [0.37454012],\n",
       "       [0.5612772 ],\n",
       "       [0.0884925 ],\n",
       "       [0.06355835],\n",
       "       [0.51423444],\n",
       "       [0.18485446],\n",
       "       [0.63755747],\n",
       "       [0.80219698],\n",
       "       [0.72900717],\n",
       "       [0.18340451],\n",
       "       [0.07455064],\n",
       "       [0.12203823],\n",
       "       [0.28093451],\n",
       "       [0.61185289],\n",
       "       [0.4937956 ],\n",
       "       [0.02058449],\n",
       "       [0.11586906],\n",
       "       [0.72960618],\n",
       "       [0.77127035],\n",
       "       [0.43194502],\n",
       "       [0.25877998],\n",
       "       [0.92187424],\n",
       "       [0.96990985],\n",
       "       [0.38867729],\n",
       "       [0.10789143],\n",
       "       [0.94888554],\n",
       "       [0.03438852],\n",
       "       [0.86617615],\n",
       "       [0.81546143],\n",
       "       [0.76078505],\n",
       "       [0.36636184],\n",
       "       [0.89482735],\n",
       "       [0.60754485],\n",
       "       [0.83244264],\n",
       "       [0.19598286],\n",
       "       [0.52006802],\n",
       "       [0.9093204 ],\n",
       "       [0.52273283],\n",
       "       [0.70807258],\n",
       "       [0.95071431],\n",
       "       [0.13949386],\n",
       "       [0.71324479],\n",
       "       [0.70685734],\n",
       "       [0.59789998],\n",
       "       [0.42754102],\n",
       "       [0.04522729],\n",
       "       [0.86310343],\n",
       "       [0.32533033],\n",
       "       [0.80839735],\n",
       "       [0.19967378],\n",
       "       [0.32518332],\n",
       "       [0.04645041],\n",
       "       [0.35846573],\n",
       "       [0.06505159],\n",
       "       [0.59241457],\n",
       "       [0.31171108],\n",
       "       [0.77096718],\n",
       "       [0.77513282],\n",
       "       [0.35675333],\n",
       "       [0.47221493],\n",
       "       [0.77224477],\n",
       "       [0.05808361],\n",
       "       [0.14092422],\n",
       "       [0.15599452],\n",
       "       [0.98688694],\n",
       "       [0.00552212],\n",
       "       [0.02541913],\n",
       "       [0.62329813],\n",
       "       [0.93949894],\n",
       "       [0.54269608],\n",
       "       [0.30424224],\n",
       "       [0.45606998],\n",
       "       [0.18182497],\n",
       "       [0.82873751],\n",
       "       [0.29214465]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.arange(N)\n",
    "np.random.shuffle(idx)\n",
    "train_idx = idx[:int(N*.8)]\n",
    "val_idx = idx[int(N*.8):]\n",
    "\n",
    "# train_idx = np.array([0,2,4])\n",
    "x_train,y_train = x[train_idx], y[train_idx]\n",
    "x_val, y_val = x[val_idx], y[val_idx]\n",
    "x_train\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step0 - 随机初始化\n",
    "现实生活中，b,w这些参数的真实值是永远不会知道的。(如果知道了，我们就不需要大费周章地训练模型了) 为了训练模型，我们需要随即初始化parameters/weights(b/w). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76284729] [0.6379667]\n"
     ]
    }
   ],
   "source": [
    "b = np.random.randn(1)\n",
    "w = np.random.randn(1)\n",
    "print(b,w)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 - 计算Prediction\n",
    "一开始，由于b,w随机生成，我们会得到bad prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.94864178],\n",
       "       [1.11163024],\n",
       "       [1.18551445],\n",
       "       [1.22983505],\n",
       "       [1.14633865],\n",
       "       [1.00179141],\n",
       "       [1.12092345],\n",
       "       [0.81930256],\n",
       "       [0.8033954 ],\n",
       "       [1.09091174],\n",
       "       [0.88077828],\n",
       "       [1.16958773],\n",
       "       [1.27462225],\n",
       "       [1.22792959],\n",
       "       [0.87985326],\n",
       "       [0.81040812],\n",
       "       [0.84070362],\n",
       "       [0.94207415],\n",
       "       [1.15318906],\n",
       "       [1.07787244],\n",
       "       [0.77597951],\n",
       "       [0.83676789],\n",
       "       [1.22831174],\n",
       "       [1.25489209],\n",
       "       [1.03841383],\n",
       "       [0.9279403 ],\n",
       "       [1.35097235],\n",
       "       [1.38161748],\n",
       "       [1.01081046],\n",
       "       [0.83167843],\n",
       "       [1.36820466],\n",
       "       [0.78478602],\n",
       "       [1.31543883],\n",
       "       [1.28308453],\n",
       "       [1.24820282],\n",
       "       [0.99657395],\n",
       "       [1.33371734],\n",
       "       [1.15044067],\n",
       "       [1.29391797],\n",
       "       [0.88787783],\n",
       "       [1.09463337],\n",
       "       [1.34296343],\n",
       "       [1.09633343],\n",
       "       [1.21457402],\n",
       "       [1.36937136],\n",
       "       [0.85183973],\n",
       "       [1.21787371],\n",
       "       [1.21379874],\n",
       "       [1.14428757],\n",
       "       [1.03560422],\n",
       "       [0.7917008 ],\n",
       "       [1.31347853],\n",
       "       [0.97039721],\n",
       "       [1.27857788],\n",
       "       [0.89023251],\n",
       "       [0.97030342],\n",
       "       [0.79248111],\n",
       "       [0.99153649],\n",
       "       [0.80434804],\n",
       "       [1.14078806],\n",
       "       [0.96170858],\n",
       "       [1.25469868],\n",
       "       [1.25735622],\n",
       "       [0.99044403],\n",
       "       [1.06410469],\n",
       "       [1.25551374],\n",
       "       [0.7999027 ],\n",
       "       [0.85275225],\n",
       "       [0.8623666 ],\n",
       "       [1.39244829],\n",
       "       [0.76637022],\n",
       "       [0.77906385],\n",
       "       [1.16049074],\n",
       "       [1.36221633],\n",
       "       [1.10906932],\n",
       "       [0.95694371],\n",
       "       [1.05380475],\n",
       "       [0.87884556],\n",
       "       [1.29155422],\n",
       "       [0.94922585]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = b + w * x_train\n",
    "yhat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2 - 计算Loss\n",
    "loss和error不一样，error用于计算单个点的误差(真实值(label)和预测值的差异)，比如第i个点的error  \n",
    "$ error_i = \\hat y_i - y_i $  \n",
    "loss,则是这些误差的聚合。\n",
    "对于回归问题，loss=mean squared error(MSE)  \n",
    "$MSE=\\frac{1}{n}\\sum\\limits_{i=1}^nerror_i^2=\\frac{1}{n}\\sum\\limits_{i=1}^n(\\hat y_i-y_i)^2=\\frac{1}{n}\\sum\\limits_{i=1}^n(b+wx_i-y_i)^2$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9712093272791501\n"
     ]
    }
   ],
   "source": [
    "error = (yhat - y_train)\n",
    "loss = (error ** 2).mean()\n",
    "error\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.  , -1.  , -1.  , ..., -1.  , -1.  , -1.  ],\n",
       "       [-0.94, -0.94, -0.94, ..., -0.94, -0.94, -0.94],\n",
       "       [-0.88, -0.88, -0.88, ..., -0.88, -0.88, -0.88],\n",
       "       ...,\n",
       "       [ 4.88,  4.88,  4.88, ...,  4.88,  4.88,  4.88],\n",
       "       [ 4.94,  4.94,  4.94, ...,  4.94,  4.94,  4.94],\n",
       "       [ 5.  ,  5.  ,  5.  , ...,  5.  ,  5.  ,  5.  ]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_range = np.linspace(true_b-3, true_b+3, 101)\n",
    "w_range = np.linspace(true_w-3, true_w+3, 101)\n",
    "bs,ws = np.meshgrid(b_range, w_range)\n",
    "ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-2.29122914, -2.23122914, -2.17122914, ...,  3.58877086,\n",
       "          3.64877086,  3.70877086],\n",
       "        [-2.27375539, -2.21375539, -2.15375539, ...,  3.60624461,\n",
       "          3.66624461,  3.72624461],\n",
       "        [-2.25628164, -2.19628164, -2.13628164, ...,  3.62371836,\n",
       "          3.68371836,  3.74371836],\n",
       "        ...,\n",
       "        [-0.5788018 , -0.5188018 , -0.4588018 , ...,  5.3011982 ,\n",
       "          5.3611982 ,  5.4211982 ],\n",
       "        [-0.56132805, -0.50132805, -0.44132805, ...,  5.31867195,\n",
       "          5.37867195,  5.43867195],\n",
       "        [-0.5438543 , -0.4838543 , -0.4238543 , ...,  5.3361457 ,\n",
       "          5.3961457 ,  5.4561457 ]],\n",
       "\n",
       "       [[-2.54671028, -2.48671028, -2.42671028, ...,  3.33328972,\n",
       "          3.39328972,  3.45328972],\n",
       "        [-2.51390766, -2.45390766, -2.39390766, ...,  3.36609234,\n",
       "          3.42609234,  3.48609234],\n",
       "        [-2.48110505, -2.42110505, -2.36110505, ...,  3.39889495,\n",
       "          3.45889495,  3.51889495],\n",
       "        ...,\n",
       "        [ 0.66794616,  0.72794616,  0.78794616, ...,  6.54794616,\n",
       "          6.60794616,  6.66794616],\n",
       "        [ 0.70074878,  0.76074878,  0.82074878, ...,  6.58074878,\n",
       "          6.64074878,  6.70074878],\n",
       "        [ 0.7335514 ,  0.7935514 ,  0.8535514 , ...,  6.6135514 ,\n",
       "          6.6735514 ,  6.7335514 ]],\n",
       "\n",
       "       [[-2.66252228, -2.60252228, -2.54252228, ...,  3.21747772,\n",
       "          3.27747772,  3.33747772],\n",
       "        [-2.62277095, -2.56277095, -2.50277095, ...,  3.25722905,\n",
       "          3.31722905,  3.37722905],\n",
       "        [-2.58301961, -2.52301961, -2.46301961, ...,  3.29698039,\n",
       "          3.35698039,  3.41698039],\n",
       "        ...,\n",
       "        [ 1.23310875,  1.29310875,  1.35310875, ...,  7.11310875,\n",
       "          7.17310875,  7.23310875],\n",
       "        [ 1.27286008,  1.33286008,  1.39286008, ...,  7.15286008,\n",
       "          7.21286008,  7.27286008],\n",
       "        [ 1.31261142,  1.37261142,  1.43261142, ...,  7.19261142,\n",
       "          7.25261142,  7.31261142]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-2.18182497, -2.12182497, -2.06182497, ...,  3.69817503,\n",
       "          3.75817503,  3.81817503],\n",
       "        [-2.17091547, -2.11091547, -2.05091547, ...,  3.70908453,\n",
       "          3.76908453,  3.82908453],\n",
       "        [-2.16000597, -2.10000597, -2.04000597, ...,  3.71999403,\n",
       "          3.77999403,  3.83999403],\n",
       "        ...,\n",
       "        [-1.11269416, -1.05269416, -0.99269416, ...,  4.76730584,\n",
       "          4.82730584,  4.88730584],\n",
       "        [-1.10178466, -1.04178466, -0.98178466, ...,  4.77821534,\n",
       "          4.83821534,  4.89821534],\n",
       "        [-1.09087516, -1.03087516, -0.97087516, ...,  4.78912484,\n",
       "          4.84912484,  4.90912484]],\n",
       "\n",
       "       [[-2.82873751, -2.76873751, -2.70873751, ...,  3.05126249,\n",
       "          3.11126249,  3.17126249],\n",
       "        [-2.77901326, -2.71901326, -2.65901326, ...,  3.10098674,\n",
       "          3.16098674,  3.22098674],\n",
       "        [-2.72928901, -2.66928901, -2.60928901, ...,  3.15071099,\n",
       "          3.21071099,  3.27071099],\n",
       "        ...,\n",
       "        [ 2.04423904,  2.10423904,  2.16423904, ...,  7.92423904,\n",
       "          7.98423904,  8.04423904],\n",
       "        [ 2.0939633 ,  2.1539633 ,  2.2139633 , ...,  7.9739633 ,\n",
       "          8.0339633 ,  8.0939633 ],\n",
       "        [ 2.14368755,  2.20368755,  2.26368755, ...,  8.02368755,\n",
       "          8.08368755,  8.14368755]],\n",
       "\n",
       "       [[-2.29214465, -2.23214465, -2.17214465, ...,  3.58785535,\n",
       "          3.64785535,  3.70785535],\n",
       "        [-2.27461597, -2.21461597, -2.15461597, ...,  3.60538403,\n",
       "          3.66538403,  3.72538403],\n",
       "        [-2.25708729, -2.19708729, -2.13708729, ...,  3.62291271,\n",
       "          3.68291271,  3.74291271],\n",
       "        ...,\n",
       "        [-0.57433412, -0.51433412, -0.45433412, ...,  5.30566588,\n",
       "          5.36566588,  5.42566588],\n",
       "        [-0.55680544, -0.49680544, -0.43680544, ...,  5.32319456,\n",
       "          5.38319456,  5.44319456],\n",
       "        [-0.53927676, -0.47927676, -0.41927676, ...,  5.34072324,\n",
       "          5.40072324,  5.46072324]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_x = x_train[0]\n",
    "dummy_yhat = bs + ws * dummy_x\n",
    "\n",
    "all_predictions = np.apply_along_axis(func1d=lambda x:bs+ws*x, axis=1, arr=x_train)\n",
    "all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 1, 1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_labels = y_train.reshape(-1,1,1)\n",
    "all_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 101, 101)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_errors = (all_predictions - all_labels)\n",
    "all_errors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 101)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_losses = (all_errors**2).mean(axis=0)\n",
    "all_losses.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3 - 计算梯度\n",
    "梯度就是偏导。\n",
    "> Gradient = how much the loss changes if ONE parameter changes a bit!\n",
    "\n",
    "\\begin{align} \\frac{\\partial MSE}{\\partial b} &= 2\\frac{1}{n}\\sum\\limits_{i=1}^n(\\hat y_i-y_i) \\\\ \n",
    "              \\frac{\\partial MSE}{\\partial w} &= 2\\frac{1}{n}\\sum\\limits_{i=1}^nx_i(\\hat y_i-y_i) \\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.7960531165831015 -1.0982700796216076\n"
     ]
    }
   ],
   "source": [
    "b_grad = 2 * error.mean()\n",
    "w_grad = 2 * (x_train * error).mean()\n",
    "print(b_grad, w_grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4 - 更新参数\n",
    "\n",
    "\\begin{align} b &= b - \\eta\\frac{\\partial MSE}{\\partial b} \\\\ \n",
    "w &= w - \\eta\\frac{\\partial MSE}{\\partial w} \\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76284729] [0.6379667]\n",
      "[0.9424526] [0.74779371]\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1 \n",
    "print(b,w)\n",
    "b = b - lr * b_grad \n",
    "w = w - lr * w_grad \n",
    "print(b,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero mean and unit standard deviation\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "scaler.fit(x_train)\n",
    "scaled_x_train = scaler.transform(x_train)\n",
    "scaled_x_val = scaler.transform(x_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5 - 漂洗和重复\n",
    "> An epoch is complete whenever every point in the training set(N) has already been used in all steps: forward pass, computing loss, computing gradients, and updating parameters.\n",
    "\n",
    "batch, mini-batch, stochastic gradient descent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
